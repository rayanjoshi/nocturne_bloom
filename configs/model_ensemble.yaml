model:
  target_sequence_length: 1  # Predicting the next closing price
  huber_delta: 2.090278310125737  # Delta for Huber loss, can be adjusted based on the dataset
  # non-normalized weighted averaging of CNN and ElasticNet regression outputs #
  
  price_loss_weight: 0.6576468910714767
  direction_loss_weight: 1.0643901466838606
  orthogonal_lambda: 88.71267542479445
  focal_alpha: 0.6219565211521811 # Alpha parameter for Focal Loss, can be adjusted based on class imbalance
  focal_gamma: 4.622938411900545  # Gamma parameter for Focal Loss, can be adjusted based on class imbalance
  focal_beta: 0.9988585788984044  # Beta parameter for Focal Loss, can be adjusted based on class imbalance
  # Direction threshold is set during feature engineering.

  use_meta_learning: true  # Enable/disable meta-learning
  include_base_losses: true  # Include base model losses in training
  meta_price_loss_weight:  0.5017565395318231
  meta_direction_loss_weight: 1.224786001971306

  price_cnn_weight: 0.8730896342605401
  direction_cnn_weight: 0.8386952208297653
  lstm_weight: 1.5955315985432164
  ridge_weight: 0.5635576137333222

  cnn_path: models/cnn_model.pth
  lstm_path: models/lstm_model.pth
  ridge_path: models/ridge_model.pth
  meta_price_path: models/meta_price_model.pth
  meta_direction_path: models/meta_direction_model.pth

cnn:
  inputChannels: 119  # Number of features (RSI, MACD, etc.)
  cnnChannels: [32, 16, 24]  # Reduced capacity to prevent overfitting
  kernelSize: [1, 1, 1]
  poolSize: [5, 3, 5]  # Pooling after each CNN layer
  poolPadding: [0, 0, 0]
  padding: [3, 0, 0]  # kernel_size // 2 for each dimension
  stride: 2
  dropout: [0.052234011633735145, 0.32545752913604054]
  outputSize: 1
  num_classes: 3

ridge:
  alpha: 14.456419059812365  # Regularization strength for Ridge regression, can be adjusted based on the dataset
  fit_intercept: True
  eps: 3.0954285028934425e-09

lstm:
  hidden_size: 256
  num_layers: 1
  dropout: 0.4746468231597993

optimiser:
  name: 'adam'
  base_lr: 6.519727352999722e-05
  meta_lr: 0.00541949872619301
  weightDecay: 1.2018092470807248e-07  # L2 regularization term 
  eps: 5.7730572441441915e-11
  amsgrad: False  # Whether to use AMSGrad variant of Adam
  schedulerMode: 'min'
  schedulerFactor: 0.2227722192439267
  schedulerPatience: 7
  schedulerMinLR: 1.3172742068637252e-05
