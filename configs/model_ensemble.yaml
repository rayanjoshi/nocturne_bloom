defaults:
  - data_module
model:
  target_sequence_length: 1  # Predicting the next closing price
  huber_delta: 0.11443956856559279  # Delta for Huber loss, can be adjusted based on the dataset
  # non-normalized weighted averaging of CNN and ElasticNet regression outputs #
  
  price_loss_weight: 0.7699239391953919
  direction_loss_weight: 0.5432210947784749
  orthogonal_lambda: 24.130334832731553
  focal_alpha: 0.6526227601968223 # Alpha parameter for Focal Loss, can be adjusted based on class imbalance
  focal_gamma: 2.0826847406626188  # Gamma parameter for Focal Loss, can be adjusted based on class imbalance
  focal_beta: 0.9988585788984044  # Beta parameter for Focal Loss, can be adjusted based on class imbalance
  # Direction threshold is set during feature engineering.

  use_meta_learning: true  # Enable/disable meta-learning
  include_base_losses: true  # Include base model losses in training
  meta_price_loss_weight:  1.957790346258156
  meta_direction_loss_weight: 1.724379023721632

  price_cnn_weight: 1.887922858910995
  direction_cnn_weight: 1.1208939594311682
  lstm_weight: 1.0748959191970795
  ridge_weight: 0.4583351676750925

  cnn_path: models/cnn_model.pth
  lstm_path: models/lstm_model.pth
  ridge_path: models/ridge_model.pth
  meta_price_path: models/meta_price_model.pth

cnn:
  output_seq_len: 1
  input_channels: 119  # Number of features (RSI, MACD, etc.)
  cnn_channels: [16, 48, 8]  # Reduced capacity to prevent overfitting
  kernel_size: [5, 7, 2]
  pool_size: [5, 2, 3]  # Pooling after each CNN layer
  pool_padding: [0, 0, 0]
  padding: [3, 3, 0]  # kernel_size // 2 for each dimension
  stride: 1
  dropout: [0.46432458059315673, 0.10085607878118658]
  output_size: 1
  num_classes: 3

ridge:
  alpha: 39.87200645314886  # Regularization strength for Ridge regression, can be adjusted based on the dataset
  fit_intercept: True
  eps: 5.285391068767814e-08

lstm:
  hidden_size: 256
  num_layers: 1
  dropout: 0.4746468231597993

optimiser:
  name: 'adam'
  base_lr: 0.001158651470636477
  meta_lr: 0.006783558000782332
  weight_decay: 4.152785493174216e-07  # L2 regularization term 
  eps: 3.539392686029228e-12
  amsgrad: False  # Whether to use AMSGrad variant of Adam
  scheduler_mode: 'min'
  scheduler_factor: 0.7428546309727503
  scheduler_patience: 10
  scheduler_min_lr: 6.926873895072113e-05
